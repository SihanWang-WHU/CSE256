{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CSE 256: NLP UCSD PA1:\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "nhqcDPz2Nf-S"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Classification with Logistic Regression and FF Networks(100points).\n",
    "\n",
    " The  goal of this assignment is  to get experience developing text classifiers with with linear models and simple feedforward\n",
    "neural networks.  You will see the standard\n",
    "pipeline used in many NLP tasks (reading in data, preprocessing, training, and testing).\n",
    "\n",
    "*  Part 1: PyTorch Basics (25 points)\n",
    "*  Part 2: Logistic Regression and Feedforward Neural Networks (60 points)\n",
    "*  Part 3: Exploration (20 points)\n",
    "\n",
    "Data. You will using a dataset of movie review snippets taken from IMDB.\n",
    "\n",
    "### <font color='blue'> Due:  April 22, 2024 at  10pm </font>\n",
    "\n",
    "###### IMPORTANT: After copying this notebook to your Google Drive, paste a link to it below. To get a publicly-accessible link, click the *Share* button at the top right, then click \"Get shareable link\" and copy the link.\n",
    "#### <font color=\"red\">Link: paste your link here:  </font>\n",
    "\n",
    "---\n",
    "**Notes:**\n",
    "\n",
    "Make sure to save the notebook as you go along.\n",
    "\n",
    "Submission instructions are located at the bottom of the notebook.\n",
    "\n",
    "The code should run fairly quickly (a couple of minutes at most even without a GPU), if it takes much longer than that, its likely that you have introduced an error."
   ],
   "metadata": {
    "id": "_hvgvtdgtSGd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mount your Google Drive to Colab\n",
    "\n",
    "**Note**: <font color=\"red\"> TODO: you need to specify your working foldername in this cell below:"
   ],
   "metadata": {
    "id": "o2rdUizL0unW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MilBq1muNa5z",
    "ExecuteTime": {
     "end_time": "2024-04-09T23:32:58.586455200Z",
     "start_time": "2024-04-09T23:32:58.554455500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\UCSD\\Classes\\3.CSE256\\CSE256\\PA1\\./\n",
      "F:\\UCSD\\Classes\\3.CSE256\\CSE256\\PA1\n"
     ]
    }
   ],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# TODO: Enter the foldername  in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cse256/assignments/PA1/'\n",
    "FOLDERNAME = './'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the absolute path\n",
    "absolute_path = os.path.join(current_directory, FOLDERNAME)\n",
    "\n",
    "print(absolute_path)\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "sys.path.append(FOLDERNAME)\n",
    "\n",
    "%cd $FOLDERNAME\n",
    "# This is later used to use the IMDB reviews\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: PyTorch Basics (25 Points)\n",
    "\n",
    "We will use PyTorch, a machine learning framework, for the programming assignmets in this course. The first part of this assigment focuses on  PyTorch and how it is used for NLP.\n",
    "If you are new to [PyTorch](https://pytorch.org), it is highly recommended to go to work through  [the 60 minute tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ],
   "metadata": {
    "id": "BG91IFlPOTE_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Question 1.1 (2.5 points)\n",
    "\n",
    "In state-of-the-art NLP, words are represented by low-dimensional vectors,  referred to as *embeddings*. When processing sequences such as sentences, movie, reviews, or entire paragraphs,   word embeddings are used to compute a vector representation of the sequence,  denoted by $\\boldsymbol{x}$. In the cell below, the embeddings for the words in the sequence \"Alice talked to\" are provided. Your task is to combine these embeddings into a single vector representation $\\boldsymbol{x}$, using  [element-wise vector addition](https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations). This method is a simple way to obtain a sequence representation, namely, it is a *continuous bag-of-words (BoW) representation* of a sequence."
   ],
   "metadata": {
    "id": "WCVZCyANT-sk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# This scipy_mode=False is used to avoid scientific notation\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "# Seed the random number generator for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "input_sequence = 'I like NLP'\n",
    "\n",
    "# Initialize an embedding matrix\n",
    "# We have a vocabulary of 5 words, each represented by a 10-dimensional embedding vector.\n",
    "embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=10)\n",
    "vocab = {'I': 0, 'like': 1, 'NLP': 2, 'classifiers': 3, '.': 4}\n",
    "\n",
    "# Convert the word to integer indices. These indices will be used to\n",
    "# retrieve the corresponding embeddings from the embedding matrix.\n",
    "# In PyTorch, operations are performed on Tensor objects, so we need to  convert\n",
    "# the list of indices to a LongTensor.\n",
    "indices = torch.LongTensor([vocab[w] for w in input_sequence.split()])\n",
    "input_sequence_embs = embeddings(indices)\n",
    "print('sequence embedding tensor size: ', input_sequence_embs.size())\n",
    "\n",
    "# The input_sequence_embs tensor contains the embeddings for each word in the input sequence.\n",
    "# The next step is to aggregate these embeddings into a single vector representation.\n",
    "# You will use element-wise addition to do this.\n",
    "# Write the code to add the embeddings element-wise and store the result in the variable \"x\".\n",
    "\n",
    "print(input_sequence_embs)\n",
    "x = torch.sum(input_sequence_embs, dim=0)\n",
    "\n",
    "### DO NOT MODIFY THE LINE BELOW\n",
    "print('input sequence embedding sum (continuous BoW): ', x)"
   ],
   "metadata": {
    "id": "HRgCs8TuWFKn",
    "ExecuteTime": {
     "end_time": "2024-04-09T23:38:44.642995100Z",
     "start_time": "2024-04-09T23:38:44.511997300Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence embedding tensor size:  torch.Size([3, 10])\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
      "          0.3223, -1.2633],\n",
      "        [ 0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959,\n",
      "          0.5667,  0.7935],\n",
      "        [ 0.5988, -1.5551, -0.3414,  1.8530,  0.7502, -0.5855, -0.1734,  0.1835,\n",
      "          1.3894,  1.5863]], grad_fn=<EmbeddingBackward>)\n",
      "input sequence embedding sum (continuous BoW):  tensor([-0.1770, -2.3993, -0.4721,  2.6568,  2.7157, -0.1408, -1.8421, -3.6277,\n",
      "         2.2783,  1.1165], grad_fn=<SumBackward1>)\n",
      "size of x: torch.Size([10])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Question 1.2 (2.5 points)\n",
    "Element-wise addition is not the best way to aggregate individual word embeddings in a sequence into a single vector representation (a process known as *composition*). State one significant limitation of using element-wise addition as a composition function for word embeddings?\n",
    "---"
   ],
   "metadata": {
    "id": "WdajAoKGVByR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### <font color=\"red\">Write your answer here (2-3 sentences) </font>\n",
    "Using element-wise addition as a composition function for aggregating individual word embeddings into a single vector representation can lead to the loss of important positional and contextual information. This method treats each word's contribution equally, ignoring the syntactic and semantic structures that give meaning to the sequence in which words appear. As a result, it can't distinguish between the nuances of different word orders or the specific roles words play within a sentence, leading to a less informative and potentially ambiguous composite representation."
   ],
   "metadata": {
    "id": "7sbir0XVVGN6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Question 1.3 (5 points)\n",
    "The [softmax function](https://pytorch.org/docs/master/nn.functional.html#softmax) is used in nearly all the neural network architectures we will look at in this course. The softmax is computed on an $n$-dimensional vector $<x_1, x_2, \\dots, x_n>$ as $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{1 \\leq j \\leq n} e^{x_j}}$. Given the   sequence representation $\\boldsymbol{x}$ we just computed, we can use the softmax function in combination with a linear projection using a matrix $W$ to transform $\\boldsymbol{x}$ into a probability distribution $p$ over the next word, expressed as $p = \\text{softmax}(W\\boldsymbol{x})$. Let's look at this in the cell below:"
   ],
   "metadata": {
    "id": "z115ktL2VQwo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize a random matrix W of size 10x5. This will serve as the weight matrix\n",
    "# for the linear projection of the vector x into a 5-dimensional space.\n",
    "W = torch.rand(10, 5)\n",
    "W = W.transpose(0, 1)\n",
    "\n",
    "# Project the vector x to a 5-dimensional space using the matrix W. This projection is achieved through\n",
    "# matrix multiplication. After the projection, apply the softmax function to the result,\n",
    "# which converts the 5-dimensional projected vector into a probability distribution.\n",
    "# You can find the softmax function in PyTorch's API (torch.nn.functional.softmax).\n",
    "# Store the resulting probability distribution in the variable \"probs\".\n",
    "\n",
    "#calculate the softmax of the dot product of W and x\n",
    "probs = torch.nn.functional.softmax(torch.matmul(W, x), dim=0)\n",
    "\n",
    "\n",
    "### DO NOT MODIFY THE BELOW LINE!\n",
    "print('probability distribution', probs)\n"
   ],
   "metadata": {
    "id": "yvLFHHd7ab9B",
    "ExecuteTime": {
     "end_time": "2024-04-09T23:39:30.791153500Z",
     "start_time": "2024-04-09T23:39:30.673153600Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability distribution tensor([0.0718, 0.0998, 0.1331, 0.6762, 0.0191], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Question 1.4 (5 points)\n",
    "\n",
    "In the example so far, we focused on a single sequence (\"I like NLP\"). However, in practical applications, it’s common to process multiple sequences simultaneously. This practice, known as *batching*, allows for more efficient use of GPU parallelism. In batching, each sequence is considered an example within a larger batch\n",
    "\n",
    "For this question, you will perform redo the previous computation, but with a batch of two sequences instead of just one. The final output of this cell should be a 2x5 matrix, where each row represents a probability distribution for a sequence. **Important: Avoid using loops in your solution, as you will lose points**. The code should be fully vectorized."
   ],
   "metadata": {
    "id": "ljIukDbfVgfx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For this example, we replicate our previous sequence indices to create a simple batch.\n",
    "# Normally, each example in the batch would be different.\n",
    "batch_indices = torch.cat(2 * [indices]).reshape((2, 3))\n",
    "batch_embs = embeddings(batch_indices)\n",
    "print('Batch embedding tensor size: ', batch_embs.size())\n",
    "\n",
    "# To process the batch, follow these steps:\n",
    "# Step 1: Aggregate the embeddings for each example in the batch into a single representation.\n",
    "# This is done through element-wise addition. Use torch.sum with the appropriate 'dim' argument\n",
    "# to sum across the sequence length (not the batch dimension).\n",
    "batch_x = torch.sum(batch_embs, dim=1)\n",
    "\n",
    "# Step 2: Project each aggregated representation into a 5-dimensional space using the matrix W.\n",
    "# This involves matrix multiplication, ensuring the resulting batch has the shape 2x5.\n",
    "batch_proj = torch.matmul(batch_x, W.t())\n",
    "\n",
    "# Step 3: Apply the softmax function to the projected representations to obtain probability distributions.\n",
    "# Each row in the output matrix should sum to 1, representing a probability distribution for each batch example.\n",
    "batch_probs = F.softmax(batch_proj, dim=1)\n",
    "\n",
    "### DO NOT MODIFY THE BELOW LINE\n",
    "print(\"Batch probability distributions:\", batch_probs)\n"
   ],
   "metadata": {
    "id": "64xEd2GIetE8",
    "ExecuteTime": {
     "end_time": "2024-04-09T23:43:39.272812100Z",
     "start_time": "2024-04-09T23:43:39.176804Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch embedding tensor size:  torch.Size([2, 3, 10])\n",
      "Batch probability distributions: tensor([[0.0718, 0.0998, 0.1331, 0.6762, 0.0191],\n",
      "        [0.0718, 0.0998, 0.1331, 0.6762, 0.0191]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##Question 1.5 (5 points)\n",
    "\n",
    "When processing a text sequence, how should the system handle words that are not present in the existing vocabulary? In the current implementation, the presence of such out-of-vocabulary words causes the code to fail, as in the cell below. To address this issue,  a simple solution is to use the special token `<UNK>`,  added to the vocabulary to serve as a placeholder for any unknown words.\n",
    "\n",
    "Modify the indexing function to ensure that it checks each word against the known vocabulary and substitutes any out-of-vocabulary words with the `<UNK>` token.  Make sure not to add  any new words  to the vocabulary  except for the `<UNK>` token. Don't forget to adjust the embedding table.\n"
   ],
   "metadata": {
    "id": "l98pYQ2biukl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "# Seed the random number generator for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "input_sequence = 'I like linear'\n",
    "\n",
    "\n",
    "# Initialize an embedding matrix\n",
    "# We have a vocabulary of 5 words, each represented by a 10-dimensional embedding vector.\n",
    "embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=10)\n",
    "vocab = {'I': 0, 'like': 1, 'NLP': 2, 'classifiers': 3, '.': 4}\n",
    "\n",
    "\n",
    "indices = torch.LongTensor([vocab[w] for w in input_sequence.split()]) ### MODIFY THIS INDEXING\n",
    "input_sequence_embs = embeddings(indices)\n",
    "print('sequence embedding tensor size: ', input_sequence_embs.size())\n",
    "\n"
   ],
   "metadata": {
    "id": "_LjvbU82is9C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "-------------------------------"
   ],
   "metadata": {
    "id": "FwX54r1DXQ-1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2: Logisitic Regression and Feedforward Neural Networks (60 points)\n",
    "\n",
    "In this part, you are going to experiment with Logistic Regression and Feedforward Neural Networks.  Run the starter code to train a two-layer fully connected neural network on the IMDB Sentiment Classification Dataset. The code provided below generates two plots that display the train accuracy and test accuracy. You will  build on code to produce different variants.\n"
   ],
   "metadata": {
    "id": "SONfUDqT2Hph"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import scipy.stats\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "########## Neural network class\n",
    "#\n",
    "# Network of two fully connected layers\n",
    "# with ReLU activation function and Softmax output\n",
    "###########################################\n",
    "\n",
    "class NN2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer.\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)  # Second fully connected layer, outputting two classes.\n",
    "\n",
    "    # Define the forward pass of the neural network.\n",
    "    # x: The input tensor.\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation function after the first layer.\n",
    "        x = self.fc2(x)  # Pass the result to the second layer.\n",
    "        x = F.softmax(x, dim=1)  # Apply Softmax to obtain output probabilities.\n",
    "        return x\n"
   ],
   "metadata": {
    "id": "s2prNOmj3d3R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "######### ReviewsDataset class\n",
    "#\n",
    "# create a dataset to be used for training and evaluation\n",
    "#########################\n",
    "# Function to read reviews from a directory\n",
    "def read_reviews(directory, num_reviews=1000):\n",
    "    reviews = []\n",
    "    for filename in os.listdir(directory)[:num_reviews]:  # Limit the number of files read\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            reviews.append(file.read())\n",
    "    return reviews\n",
    "\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, pos_dir, neg_dir, num_reviews=1000, vectorizer=None, train=True):\n",
    "        self.reviews = read_reviews(pos_dir, num_reviews) + read_reviews(neg_dir, num_reviews)\n",
    "        self.labels = [1] * min(num_reviews, len(os.listdir(pos_dir))) + [0] * min(num_reviews, len(os.listdir(neg_dir)))\n",
    "        if train or vectorizer is None:\n",
    "            self.vectorizer = CountVectorizer(max_features=512)  # Adjust as needed\n",
    "            self.embeddings = self.vectorizer.fit_transform(self.reviews).toarray()\n",
    "        else:\n",
    "            self.vectorizer = vectorizer\n",
    "            self.embeddings = self.vectorizer.transform(self.reviews).toarray()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n"
   ],
   "metadata": {
    "id": "wWeUuQ5n370U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "########## train_epoch\n",
    "#\n",
    "# function that trains for one epoch (one pass through the training set)\n",
    "######################\n",
    "\n",
    "def train_epoch(data_loader, model, loss_fn, optimizer):\n",
    "    size = len(data_loader.dataset)\n",
    "    num_batches = len(data_loader)\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        X = X.float()\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_train_loss = train_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    return accuracy, average_train_loss\n",
    "\n",
    "\n",
    "########## eval_epoch\n",
    "#\n",
    "# function that evaluates a model with a test set\n",
    "######################\n",
    "def eval_epoch(data_loader, model, loss_fn, optimizer):\n",
    "    size = len(data_loader.dataset)\n",
    "    num_batches = len(data_loader)\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Compute prediction error\n",
    "        X = X.float()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        eval_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    average_eval_loss = eval_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    return accuracy, average_eval_loss\n",
    "\n",
    "\n",
    "\n",
    "########## experiment\n",
    "#\n",
    "# function that trains a neural network with a training set\n",
    "# and evaluates the neural network with a test set\n",
    "#####################\n",
    "def experiment(model):\n",
    "\n",
    "\t# negative log likelihood loss function\n",
    "\tloss_fn = nn.NLLLoss()\n",
    "\n",
    "\t# Adam optimizer\n",
    "\toptimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "\taverage_train_loss = []\n",
    "\tall_train_accuracy = []\n",
    "\taverage_test_loss = []\n",
    "\tall_test_accuracy = []\n",
    "\tfor epoch in range(150):\n",
    "\t\ttrain_accuracy, train_loss = train_epoch(train_loader, model, loss_fn, optimizer)\n",
    "\t\tall_train_accuracy += [train_accuracy]\n",
    "\t\ttest_accuracy, test_loss = eval_epoch(test_loader, model, loss_fn, optimizer)\n",
    "\t\tall_test_accuracy += [test_accuracy]\n",
    "\t\tif epoch % 10 == 9:\n",
    "\t\t\tprint(f'Epoch #{epoch+1}: \\t train accuracy {train_accuracy:.3f}\\t train loss {train_loss:.3f}\\t test accuracy {test_accuracy:.3f}\\t test loss {test_loss:.3f}')\n",
    "\treturn all_train_accuracy, all_test_accuracy\n"
   ],
   "metadata": {
    "id": "Y_EXXh3N4X9c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "######################\n",
    "#\n",
    "# 1) Load data splits: the train and test sets\n",
    "# 2) Train neural networks\n",
    "# 3) Plot the results\n",
    "############################\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the dataset\n",
    "root_dir = '/content/drive/My Drive/{}/aclImdb/train'.format(FOLDERNAME)\n",
    "root_dir_test = '/content/drive/My Drive/{}/aclImdb/test'.format(FOLDERNAME)\n",
    "train_dataset = ReviewsDataset(root_dir+'/pos', root_dir+'/neg', train=True)\n",
    "test_dataset = ReviewsDataset(root_dir_test+'/pos', root_dir_test+'/neg', vectorizer=train_dataset.vectorizer, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Time to load data: {elapsed_time} seconds\")\n",
    "\n"
   ],
   "metadata": {
    "id": "APmkf7PW4Xmp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# train neural networks\n",
    "print('\\n2 layers:')\n",
    "nn2_train_accuracy, nn2_test_accuracy = experiment(NN2(input_size=512, hidden_size=100))\n",
    "\n",
    "# plot training accuracy\n",
    "plt.plot(nn2_train_accuracy)\n",
    "plt.title('training accuracy (varying # of layers)')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['2 layers'])\n",
    "plt.show()\n",
    "\n",
    "# plot testing accuracy\n",
    "plt.plot(nn2_test_accuracy)\n",
    "plt.title('testing accuracy (varying # of layers)')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['2 layers'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Time to train, eval model: {elapsed_time} seconds\")"
   ],
   "metadata": {
    "id": "qWoR_WEi4W82"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <font color=\"red\">  TO DO: Impelementation\n",
    "\n",
    "- Implement and test fully connected networks with 1,2,3, and 4 layers. The starter code above already provides you with an implementation of 2 layers. Each hidden layer should have 100 nodes.\n",
    "-  On the four layer network, modify the code to replace the ReLU activation function with the sigmoid activation function.\n",
    "- On the four layer network, modify your code to insert a dropout layer with probability 0.5 after each hidden leayer. Tip: see the function nn.dropout().\n"
   ],
   "metadata": {
    "id": "W0jH5XpXALKp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### YOUR CODE HERE"
   ],
   "metadata": {
    "id": "F75cGRwfZ236"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2.1 Architecture Comparison (20 points)\n",
    "\n",
    "Generate two plots where the y-axis is the accuracy and the x-axis is the # of epochs. The first plot should include 4 curves that show the training accuracy for 1, 2, 3, and 4 layers. The second plot should include 4 curves that show the testing accuracy for 1, 2, 3, and 4 layers. Use ReLU activation functions without any dropout and 100 nodes per hidden layer. Discuss the results."
   ],
   "metadata": {
    "id": "on4YzOrlaGQC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### YOUR CODE HERE"
   ],
   "metadata": {
    "id": "zD00a4X8aYoe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### <font color=\"red\">Analysis and discussion here (< 5 sentences) :</font>"
   ],
   "metadata": {
    "id": "NwG7ir96hQXc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2.2 Activation functions (20 points)\n",
    "\n",
    " Generate two plots where the y-axis is the accuracy and the x-axis is the # of epochs. The first plot should include 2 curves that show the training accuracy when using the ReLU versus sigmoid activation functions. The second plot should include 2 curves that show the testing accuracy when using the ReLU versus sigmoid activation functions. Use 2 layers and 100 nodes per hidden layer without any dropout. Discuss the results."
   ],
   "metadata": {
    "id": "wQofMUeOatU7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### YOUR CODE HERE"
   ],
   "metadata": {
    "id": "hc_1_SI1aq21"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "msHzyt_YswqL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### <font color=\"red\">Analysis and discussion here (< 5 sentences) :</font>"
   ],
   "metadata": {
    "id": "83-0geAMhfKs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2.3 Dropout comparison (15 points)\n",
    "\n",
    "Generate two plots where the y-axis is the accuracy and the x-axis is the # of epochs. The first plot should include 2 curves that show the training accuracy with and without dropout (with probability 0.5) after each hidden layer. The second plot should include 2 curves that show the testing accuracy with and without dropout (with probability 0.5) after each hidden layer. Use 4 layers and 36 nodes per hidden layer with ReLU activation functions. Discuss the results."
   ],
   "metadata": {
    "id": "LjfLC7L4bJDj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### YOUR CODE HERE"
   ],
   "metadata": {
    "id": "Yev_e7orbjBq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### <font color=\"red\">Analysis and discussion here (< 5 sentences) :</font>\n"
   ],
   "metadata": {
    "id": "mOLh3rK6hhE2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2.4  (5 points)\n",
    "\n",
    "Now that you have trained a Neural Network classifier, you may find that your testing accuracy is much lower than the training accuracy. In what ways can we decrease this gap? Pick all that apply.\n",
    "\n",
    "1. Train on a larger dataset.\n",
    "2. Add more hidden units.\n",
    "3. Increase the regularization strength.\n",
    "4. None of the above."
   ],
   "metadata": {
    "id": "jsElccrAf9uE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#### <font color=\"red\">Answer here </font>:\n",
    "\n",
    "\n",
    "\n",
    "#### <font color=\"red\">Explanation (< 5 sentences) here: </font>:\n"
   ],
   "metadata": {
    "id": "FOCo-0XIgbem"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3: Exploration  (20 points)"
   ],
   "metadata": {
    "id": "090-JGk7h_wQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 3.1 Explore  (20 points)\n",
    "\n",
    "There are other aspects to optimizing neural network performance. Explore two here, and discuss your findings. You may also try different neural architures here, other than feedforward networks."
   ],
   "metadata": {
    "id": "2drhmJ2XosCj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### YOUR CODE HERE"
   ],
   "metadata": {
    "id": "eRsbWVpKsZjk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### <font color=\"red\">Analysis and discussion here (< 15 sentences) :</font>\n"
   ],
   "metadata": {
    "id": "vQGJ6DqAfsrB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------------------\n",
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "---------------------------\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of all cells).\n",
    "2. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "3. Once you've rerun everything, select File -> Download as -> PDF via LaTeX (If you have trouble using \"PDF via LaTex\", you can also save the webpage as pdf. <font color='blue'> Make sure all your solutions  are displayed in the pdf</font>, it's okay if the provided codes get cut off because lines are not wrapped in code cells).\n",
    "4. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing your graders will see!\n",
    "5. Submit your PDF on Gradescope.\n"
   ],
   "metadata": {
    "id": "YfuoIQajhtQd"
   }
  }
 ]
}
